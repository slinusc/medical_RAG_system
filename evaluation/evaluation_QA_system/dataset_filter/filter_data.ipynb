{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we loop trough each training set for example BioASQ-trainingDataset2b.json and extract the pubmed IDS used to answers questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON Files: 100%|██████████| 11/11 [00:09<00:00,  1.21it/s]\n",
      "Aggregating PubMed IDs: 100%|██████████| 43188/43188 [00:00<00:00, 3358304.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "csv_dir = os.path.expanduser(json_dir + '/csv')  # Ensure the path is expanded to the user's home directory\n",
    "\n",
    "# Create the CSV directory if it doesn't exist\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a set to hold all unique PubMed IDs across files\n",
    "all_pubmed_ids = set()\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]  # Ensure the path is expanded\n",
    "\n",
    "# Loop through files with a tqdm progress bar\n",
    "for json_file in tqdm(json_files, desc=\"Processing JSON Files\"):\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "\n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize a set for this file's PubMed IDs\n",
    "    file_pubmed_ids = set()\n",
    "\n",
    "    # Extract unique PubMed IDs from the 'questions' section\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        for url in documents:\n",
    "            pubmed_id = int(url.split('/')[-1])\n",
    "            file_pubmed_ids.add(pubmed_id)\n",
    "\n",
    "    # Update the set of all PubMed IDs, since a set can only contain unique numbers the same  PUBMEDIDS wont be stored twice\n",
    "    all_pubmed_ids.update(file_pubmed_ids)\n",
    "\n",
    "    # Save to DataFrame and CSV for this file\n",
    "    df = pd.DataFrame({'pubmedid': list(file_pubmed_ids)})\n",
    "    csv_filename = os.path.splitext(json_file)[0] + '.csv'\n",
    "    csv_path = os.path.join(csv_dir, csv_filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Convert the set to a list with tqdm progress\n",
    "all_pubmed_ids_list = list(tqdm(all_pubmed_ids, desc=\"Aggregating PubMed IDs\"))\n",
    "\n",
    "# Save all PubMed IDs to a DataFrame with an extra column and save to CSV\n",
    "all_pubmed_ids_df = pd.DataFrame({'pubmedid': all_pubmed_ids_list, 'enthalten_in_dataset': 0})\n",
    "complete_csv_path = os.path.join(csv_dir, 'csv_complete.csv')\n",
    "all_pubmed_ids_df.to_csv(complete_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we read in all the pubmedids we currently have loaded into our "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script reads in the before created list of all pubmed ids used to answer questions and all the pubmed ids used in our dataset\n",
    "\n",
    "it then flags all  the pubmedids which are avaible in the questions and our data subset used (remember we created the latter in the previous stepp)\n",
    "\n",
    "in the end we first update the csv_complete.csv on wether or not the pubmedid is containted flag afterwards we save the matched pubmed ids into a seperate file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of PubMed IDs with a 1: 3.035565434843012%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "complete_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/csv_complete.csv' #csv file with all unique pubmed ids that are used to answer questions\n",
    "rag_pubmed_csv_path = '~/data/faiss_indices/bioBERT/PMIDs/concatenated_pubmed_ids.csv' #csv file with all the pubmedids currently in our system\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv' # csv file that results from running this script which containes alll the pubmed ids that \n",
    "# should be able to answer questions \n",
    "\n",
    "# Read the DataFrames\n",
    "complete_df = pd.read_csv(complete_csv_path)\n",
    "# Read the RAGPubMed.csv file assuming it has no header and only one column of integers\n",
    "rag_pubmed_df = pd.read_csv(rag_pubmed_csv_path, header=None, names=['PMID'], dtype={'PMID': int})\n",
    "\n",
    "\n",
    "# Check for presence and update the column\n",
    "complete_df['enthalten_in_dataset'] = complete_df['pubmedid'].isin(rag_pubmed_df['PMID']).astype(int)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "complete_df.to_csv(complete_csv_path, index=False)\n",
    "\n",
    "# Extract the PubMed IDs that have a match (1 in the 'enthalten_in_dataset' column)\n",
    "matched_pubmed_ids = complete_df[complete_df['enthalten_in_dataset'] == 1]['pubmedid']\n",
    "\n",
    "# Save the matched PubMed IDs to a separate CSV file\n",
    "matched_pubmed_ids.to_csv(matched_ids_csv_path, index=False, header=['pubmedid'])\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (complete_df['enthalten_in_dataset'].sum() / len(complete_df)) * 100\n",
    "\n",
    "print(f\"Percentage of PubMed IDs with a 1: {percentage}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract each questions that has at least one pubmed id as answer which is present in our dataset and save it into the json file. we also provide the count of how many questions should be answerable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in all JSON files: 30212\n",
      "Total selected entries saved: 723\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv'\n",
    "output_json_path = '~/Questions_answers_data/all_questions_in_system.json'\n",
    "\n",
    "# Read the matched PubMed IDs\n",
    "matched_ids_df = pd.read_csv(matched_ids_csv_path)\n",
    "matched_pubmed_ids = set(matched_ids_df['pubmedid'])\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]\n",
    "\n",
    "# Initialize a list to hold entries that meet the criteria and a counter for all entries\n",
    "selected_entries = []\n",
    "total_entries = 0\n",
    "\n",
    "# Loop through each JSON file\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "    \n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Increment total_entries by the number of questions in the current file\n",
    "    total_entries += len(data.get('questions', []))\n",
    "    \n",
    "    # Check each entry for matched PubMed IDs\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        pubmed_ids = [int(url.split('/')[-1]) for url in documents]\n",
    "        # Count how many PubMed IDs in this question are in the matched list\n",
    "        match_count = sum(id_ in matched_pubmed_ids for id_ in pubmed_ids)\n",
    "        # If at least one (or two) match(es), save the entire entry\n",
    "        if match_count >= 1:  # Change to `>= 2` if you need at least two matches ore more\n",
    "            selected_entries.append(question)\n",
    "\n",
    "# Save the selected entries to a new JSON file\n",
    "with open(os.path.expanduser(output_json_path), 'w') as file:\n",
    "    json.dump({'questions': selected_entries}, file, indent=4)\n",
    "\n",
    "# Print the total count of entries and the count of selected entries\n",
    "print(f\"Total entries in all JSON files: {total_entries}\")\n",
    "print(f\"Total selected entries saved: {len(selected_entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note for later:\n",
    "\n",
    "if only one pubmed id required we can answer 4000 questions\n",
    "with at least 2 we can answer 2036\n",
    "with at least 3 1221\n",
    "with at least 4 731\n",
    "with all 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script above does exactly the same as above but only counts the question if all the pubmed ids to answer are in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in all JSON files: 30212\n",
      "Total selected entries saved: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "json_dir = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets'\n",
    "matched_ids_csv_path = '~/Questions_answers_data/DATEN_RAG_PM4/trainings_sets/csv/matched_pubmed_ids.csv'\n",
    "output_json_path = '~/Questions_answers_data/all_questions_with_all_ids_matched.json'  # Updated file name\n",
    "\n",
    "# Read the matched PubMed IDs\n",
    "matched_ids_df = pd.read_csv(matched_ids_csv_path)\n",
    "matched_pubmed_ids = set(matched_ids_df['pubmedid'])\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(os.path.expanduser(json_dir)) if f.endswith('.json')]\n",
    "\n",
    "# Initialize a list to hold entries that meet the criteria and a counter for all entries\n",
    "selected_entries = []\n",
    "total_entries = 0\n",
    "\n",
    "# Loop through each JSON file\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(os.path.expanduser(json_dir), json_file)\n",
    "    \n",
    "    # Load JSON content\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Increment total_entries by the number of questions in the current file\n",
    "    total_entries += len(data.get('questions', []))\n",
    "    \n",
    "    # Check each entry for matched PubMed IDs\n",
    "    for question in data.get('questions', []):\n",
    "        documents = question.get('documents', [])\n",
    "        pubmed_ids = [int(url.split('/')[-1]) for url in documents]\n",
    "        # Check if all PubMed IDs in this question are in the matched list\n",
    "        if all(id_ in matched_pubmed_ids for id_ in pubmed_ids):\n",
    "            selected_entries.append(question)\n",
    "\n",
    "# Save the selected entries to a new JSON file\n",
    "with open(os.path.expanduser(output_json_path), 'w') as file:\n",
    "    json.dump({'questions': selected_entries}, file, indent=4)\n",
    "\n",
    "# Print the total count of entries and the count of selected entries\n",
    "print(f\"Total entries in all JSON files: {total_entries}\")\n",
    "print(f\"Total selected entries saved: {len(selected_entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since i didnt think about it that much before we now proceed to remove the duplicates from all generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before: 298, Entries after: 298, Duplicates removed: 0\n",
      "Entries per type: {'factoid': 91, 'summary': 57, 'yesno': 87, 'list': 63}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_duplicates(json_file_path):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the questions list\n",
    "    questions = data.get(\"questions\", [])\n",
    "\n",
    "    # Store unique questions and count types\n",
    "    unique_questions = {}\n",
    "    type_counts = {}\n",
    "    for question in questions:\n",
    "        question_id = question.get(\"id\")\n",
    "        question_type = question.get(\n",
    "            \"type\", \"unknown\"\n",
    "        )  # Handling cases where type might not be present\n",
    "\n",
    "        # Count the types of questions\n",
    "        if question_type in type_counts:\n",
    "            type_counts[question_type] += 1\n",
    "        else:\n",
    "            type_counts[question_type] = 1\n",
    "\n",
    "        # Add unique questions\n",
    "        if question_id not in unique_questions:\n",
    "            unique_questions[question_id] = question\n",
    "\n",
    "    # Calculate the numbers for reporting\n",
    "    initial_count = len(questions)\n",
    "    final_count = len(unique_questions)\n",
    "    removed_count = initial_count - final_count\n",
    "\n",
    "    # Save the unique questions back to the file\n",
    "    data[\"questions\"] = list(unique_questions.values())\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Return the counts and type summary\n",
    "    return {\n",
    "        \"initial_count\": initial_count,\n",
    "        \"final_count\": final_count,\n",
    "        \"removed_count\": removed_count,\n",
    "        \"type_counts\": type_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "path = \"/home/ubuntu/questions_answers_data/all_questions_in_system_min2.json\"\n",
    "result = remove_duplicates(path)\n",
    "print(f\"Entries before: {result['initial_count']}, Entries after: {result['final_count']}, \"\n",
    "      f\"Duplicates removed: {result['removed_count']}\")\n",
    "print(\"Entries per type:\", result['type_counts'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before: 4064, Entries after: 593, Duplicates removed: 3471\n",
      "Entries per type: {'yesno': 1145, 'factoid': 1190, 'list': 900, 'summary': 829}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_duplicates(json_file_path):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the questions list\n",
    "    questions = data.get(\"questions\", [])\n",
    "\n",
    "    # Store unique questions and count types\n",
    "    unique_questions = {}\n",
    "    type_counts = {}\n",
    "    for question in questions:\n",
    "        question_id = question.get(\"id\")\n",
    "        question_type = question.get(\n",
    "            \"type\", \"unknown\"\n",
    "        )  # Handling cases where type might not be present\n",
    "\n",
    "        # Count the types of questions\n",
    "        if question_type in type_counts:\n",
    "            type_counts[question_type] += 1\n",
    "        else:\n",
    "            type_counts[question_type] = 1\n",
    "\n",
    "        # Add unique questions\n",
    "        if question_id not in unique_questions:\n",
    "            unique_questions[question_id] = question\n",
    "\n",
    "    # Calculate the numbers for reporting\n",
    "    initial_count = len(questions)\n",
    "    final_count = len(unique_questions)\n",
    "    removed_count = initial_count - final_count\n",
    "\n",
    "    # Save the unique questions back to the file\n",
    "    data[\"questions\"] = list(unique_questions.values())\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Return the counts and type summary\n",
    "    return {\n",
    "        \"initial_count\": initial_count,\n",
    "        \"final_count\": final_count,\n",
    "        \"removed_count\": removed_count,\n",
    "        \"type_counts\": type_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "path = \"/home/ubuntu/questions_answers_data/all_questions_in_system.json\"\n",
    "result = remove_duplicates(path)\n",
    "print(\n",
    "    f\"Entries before: {result['initial_count']}, Entries after: {result['final_count']}, \"\n",
    "    f\"Duplicates removed: {result['removed_count']}\"\n",
    ")\n",
    "print(\"Entries per type:\", result[\"type_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before: 593, Entries after: 593, Duplicates removed: 0\n",
      "Entries per type: {'yesno': 160, 'factoid': 179, 'list': 131, 'summary': 123}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def remove_duplicates(json_file_path):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the questions list\n",
    "    questions = data.get(\"questions\", [])\n",
    "\n",
    "    # Store unique questions and count types\n",
    "    unique_questions = {}\n",
    "    type_counts = {}\n",
    "    for question in questions:\n",
    "        question_id = question.get(\"id\")\n",
    "        question_type = question.get(\n",
    "            \"type\", \"unknown\"\n",
    "        )  # Handling cases where type might not be present\n",
    "\n",
    "        # Count the types of questions\n",
    "        if question_type in type_counts:\n",
    "            type_counts[question_type] += 1\n",
    "        else:\n",
    "            type_counts[question_type] = 1\n",
    "\n",
    "        # Add unique questions\n",
    "        if question_id not in unique_questions:\n",
    "            unique_questions[question_id] = question\n",
    "\n",
    "    # Calculate the numbers for reporting\n",
    "    initial_count = len(questions)\n",
    "    final_count = len(unique_questions)\n",
    "    removed_count = initial_count - final_count\n",
    "\n",
    "    # Save the unique questions back to the file\n",
    "    data[\"questions\"] = list(unique_questions.values())\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Return the counts and type summary\n",
    "    return {\n",
    "        \"initial_count\": initial_count,\n",
    "        \"final_count\": final_count,\n",
    "        \"removed_count\": removed_count,\n",
    "        \"type_counts\": type_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "path = \"/home/ubuntu/questions_answers_data/all_questions_in_system.json\"\n",
    "result = remove_duplicates(path)\n",
    "print(\n",
    "    f\"Entries before: {result['initial_count']}, Entries after: {result['final_count']}, \"\n",
    "    f\"Duplicates removed: {result['removed_count']}\"\n",
    ")\n",
    "print(\"Entries per type:\", result[\"type_counts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
