{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we import some neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from RAG_evaluator import RAG_evaluator\n",
    "sys.path.append(\"../../rag_system/\")\n",
    "from med_rag import MedRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we define an experiment name, this name should ! uniqely! identify the experiemnal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"experiment_4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we implement a running experiment by using rag system one two and three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'experiment_4' already exists at /home/ubuntu/questions_answers_data/experiment_results/experiment_4\n"
     ]
    }
   ],
   "source": [
    "# Base directory where the new folder will be created\n",
    "base_directory = \"/home/ubuntu/questions_answers_data/experiment_results\"\n",
    "# input directory, change if diffrent one is used\n",
    "question_input_dir =   \"/home/ubuntu/questions_answers_data/all_questions_in_system_min4.json\"\n",
    "\n",
    "\n",
    "# Construct the path for the new experiment folder\n",
    "experiment_folder_path = os.path.join(base_directory, experiment_name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(experiment_folder_path):\n",
    "    os.makedirs(experiment_folder_path)\n",
    "    print(f\"Directory '{experiment_name}' created at {experiment_folder_path}\")\n",
    "else:\n",
    "    print(f\"Directory '{experiment_name}' already exists at {experiment_folder_path}\")\n",
    "\n",
    "# Construct the path for the JSON file\n",
    "output_path_retriever_1 = os.path.join(experiment_folder_path, \"result_ragver_1.json\")\n",
    "output_path_retriever_2 = os.path.join(experiment_folder_path, \"result_ragver_2.json\")\n",
    "output_path_retriever_3 = os.path.join(experiment_folder_path, \"result_ragver_3.json\")\n",
    "output_path_retriever_4 = os.path.join(experiment_folder_path, \"result_ragver_4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the 3 retriever types used in the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through all questions which are possible to answer with our 10% corpus, and saving the results as a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_1 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_1,\n",
    ")\n",
    "rag_type_1.run_eval(retriever_type=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 1 correct PMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RAG_evaluator.__init__() missing 1 required positional argument: 'output_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rag_type_1 \u001b[38;5;241m=\u001b[39m \u001b[43mRAG_evaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_input_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path_retriever_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m rag_type_1\u001b[38;5;241m.\u001b[39manalyze_performance(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/questions_answers_data/experiment_results/experiment_4/result_ragver_1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: RAG_evaluator.__init__() missing 1 required positional argument: 'output_path'"
     ]
    }
   ],
   "source": [
    "rag_type_1 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_1,\n",
    ")\n",
    "rag_type_1.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_4/result_ragver_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 4 correct PMIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 1\n",
      "Total Questions: 239\n",
      "\n",
      "Response Time:\n",
      "Mean: 1.98 seconds\n",
      "Standard Deviation: 0.51 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.49\n",
      "Recall: 0.35\n",
      "Precision: 0.32\n",
      "F1 Score: 0.25\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.09\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_1.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_2/result_ragver_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rag Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through all questions which are possible to answer with our 10% corpus, and saving the results as a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_2 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_2.run_eval(retriever_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 1 correct PMID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 2\n",
      "Total Questions: 989\n",
      "\n",
      "Response Time:\n",
      "Mean: 2.46 seconds\n",
      "Standard Deviation: 0.85 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.70\n",
      "Recall: 0.30\n",
      "Precision: 0.26\n",
      "F1 Score: 0.26\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.28\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_2.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_1/result_ragver_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the results of the evaluation of all questions containing at least 4 correct PMIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 2\n",
      "Total Questions: 239\n",
      "\n",
      "Response Time:\n",
      "Mean: 2.47 seconds\n",
      "Standard Deviation: 0.89 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.68\n",
      "Recall: 0.32\n",
      "Precision: 0.35\n",
      "F1 Score: 0.33\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 1.00\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_2.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_2/result_ragver_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rag Type 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_3 = RAG_evaluator(\n",
    "    question_input_dir,\n",
    "    output_path_retriever_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_type_3.run_eval(retriever_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 3\n",
      "Total Questions: 239\n",
      "\n",
      "Response Time:\n",
      "Mean: 6.00 seconds\n",
      "Standard Deviation: 10.04 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.51\n",
      "Recall: 0.35\n",
      "Precision: 0.33\n",
      "F1 Score: 0.27\n",
      "\n",
      "Additional Summary Statistics:\n",
      "Average Number of PubMed IDs Returned: 0.11\n",
      "Average Number of PubMed IDs Retrieved: 0.00\n"
     ]
    }
   ],
   "source": [
    "rag_type_3.analyze_performance(\"/home/ubuntu/questions_answers_data/experiment_results/experiment_2/result_ragver_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = MedRAG(retriever=4, question_type=2)\n",
    "\n",
    "rag_type_4 = RAG_evaluator(\n",
    "    rag_model=rag_system,\n",
    "    path_to_question_json=question_input_dir,\n",
    "    output_path=output_path_retriever_4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   4%|‚ñç         | 28/723 [00:45<18:55,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results written to /home/ubuntu/questions_answers_data/experiment_results/experiment_4/result_ragver_4.json\n",
      "Processing time: 45.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag_type_4.run_eval(retriever_type=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 4\n",
      "Total Questions: 11\n",
      "\n",
      "Response Time:\n",
      "Mean: 4.16 seconds\n",
      "Standard Deviation: 1.22 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.64\n",
      "Recall: 0.41\n",
      "Precision: 0.35\n",
      "F1 Score: 0.34\n"
     ]
    }
   ],
   "source": [
    "rag_type_4.analyze_performance(\n",
    "    \"/home/ubuntu/questions_answers_data/experiment_results/experiment_4/result_ragver_4.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score ,f1_score , precision_score\n",
    "def manual_accuracy_score(y_true, y_pred):\n",
    "    # Ensure that y_true and y_pred are the same length\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\n",
    "            \"The length of true labels and predicted labels must be the same.\"\n",
    "        )\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total observations\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def analyze_performance(json_file_path):\n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Convert JSON data into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # extract the retriever number from the file path\n",
    "    retriever = re.search(r\"ragver_(\\d+)\", json_file_path).group(1)\n",
    "    print(f\"Summary Statistics for RAG with retriever {retriever}\")\n",
    "    print(f\"Total Questions: {len(df)}\")\n",
    "\n",
    "    # Calculate mean and standard deviation for the response time\n",
    "    mean_response_time = df[\"requestime\"].mean()\n",
    "    sd_response_time = df[\"requestime\"].std()\n",
    "    print(\"\\nResponse Time:\")\n",
    "    print(\"Mean: {:.2f} seconds\".format(mean_response_time))\n",
    "    print(\"Standard Deviation: {:.2f} seconds\".format(sd_response_time))\n",
    "\n",
    "    # Calculate accuracy, recall, precision, and F1-score\n",
    "    # Assuming 'actual' and 'predicted' are the column names for your true and predicted binary classification outcomes\n",
    "    accuracy = manual_accuracy_score(df[\"trueresponse_exact\"], df[\"ragresponse\"])\n",
    "    recall = recall_score(\n",
    "        df[\"trueresponse_exact\"],\n",
    "        df[\"ragresponse\"],\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    precision = precision_score(\n",
    "        df[\"trueresponse_exact\"],\n",
    "        df[\"ragresponse\"],\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    f1 = f1_score(\n",
    "        df[\"trueresponse_exact\"],\n",
    "        df[\"ragresponse\"],\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    print(\"\\nClassification Metrics:\")\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "    print(\"Recall: {:.2f}\".format(recall))\n",
    "    print(\"Precision: {:.2f}\".format(precision))\n",
    "    print(\"F1 Score: {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for RAG with retriever 4\n",
      "Total Questions: 11\n",
      "\n",
      "Response Time:\n",
      "Mean: 4.16 seconds\n",
      "Standard Deviation: 1.22 seconds\n",
      "\n",
      "Classification Metrics:\n",
      "Accuracy: 0.64\n",
      "Recall: 0.64\n",
      "Precision: 0.80\n",
      "F1 Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "analyze_performance(\n",
    "    \"/home/ubuntu/questions_answers_data/experiment_results/experiment_4/result_ragver_4.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
